<?xml version="1.0" encoding="UTF-8"?>


<section xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="section-fields-vectorspaces">
  <title>Extension Fields as Vector Spaces</title>
  
  <introduction>
    <p>
      To further analyze extension fields, we must shift our perspective.  Instead of considering extension fields as a type of field, we will consider them instead as a <em>vector space</em>.  (This is not crazy: we did a similar thing when trying to understand the integers.  We could consider them as a group, but when we consider them as a ring, we learn more.)
    </p>
    
    <p>
      Here is the plan: we will briefly review the main ideas about vector spaces in general, in case it has been a while since you thought about the topic.  Then we will leverage the machinery of linear algebra to discover new results about extension fields.
    </p>
  </introduction>
  
  <subsection xml:id="subsec-fields-vectorspaces-review">
    <title>A Review of Vector Spaces</title>
    <introduction permid="LVx">
      <p>
        In a linear algebra class, you probably thought of vectors primarily as <em>column</em> vectors: lists of tuples such as <m>\begin{bmatrix}3\\0\\1\end{bmatrix}</m> of <m>\begin{bmatrix}4\\-2\end{bmatrix}</m>.  These are exaples of vectors in <m>\R^3</m> and <m>\R^2</m>, respectively, and <m>\R^3</m> and <m>\R^2</m> are indeed examples of vector <em>spaces</em>. 
      </p>
      
      <p>
        The more abstract approach is to consider a set together with some operations that must satisfy some axioms.  Then, just like we saw with groups and rings, we can generalize what works for those generic vectors to other mathematical objects.  For example, the set of all polynomials of degree no more than 2 looks very much like <m>\R^3</m> as a vector space.  
      </p>
      
      <p>
        Our goal is to view an extension field as a vector space, so let's say carefully what a vector space is using the abstract, axiomatic approach.
      </p>
    </introduction>

    <subsubsection xml:id="section-vect-definitions-and-examples" permid="Ogj">
      <title>Definitions and Examples</title>
      <p permid="Vbb">
        A <term>vector space</term><idx><h>Vector space</h><h>definition of</h></idx> <m>V</m> over a field <m>F</m> is an abelian group with a
        <term>scalar product</term>
            <idx><h>Scalar product</h></idx>
        <m>\alpha \cdot v</m> or <m>\alpha v</m> defined for all
        <m>\alpha \in F</m> and all <m>v \in V</m> satisfying the following axioms.

        <ul permid="GQf">
          <li permid="mXo">
            <p permid="POs">
              <m>\alpha(\beta v) =(\alpha \beta)v</m>;
            </p>
          </li>

          <li permid="Tex">
            <p permid="vVB">
              <m>(\alpha + \beta)v =\alpha v + \beta v</m>;
            </p>
          </li>

          <li permid="zlG">
            <p permid="ccK">
              <m>\alpha(u + v) = \alpha u + \alpha v</m>;
            </p>
          </li>

          <li permid="fsP">
            <p permid="IjT">
              <m>1v=v</m>;
            </p>
          </li>
        </ul>

        where <m>\alpha, \beta \in F</m> and <m>u, v \in V</m>.
      </p>

      <p permid="Bik">
        The elements of <m>V</m> are called <term>vectors</term>;
        the elements of <m>F</m> are called <term>scalars</term>.
        It is important to notice that in most cases two vectors cannot be multiplied.
        In general, it is only possible to multiply a vector with a scalar.
        To differentiate between the scalar zero and the vector zero,
        we will write them as 0 and <m>{\mathbf 0}</m>, respectively.
      </p>

      <p permid="hpt">
        Let us examine several examples of vector spaces.
        Some of them will be quite familiar;
        others will seem less so.
      </p>

      <example xml:id="example-vect-space0-rn" permid="OQd">
        <p permid="sjX">
          The <m>n</m>-tuples of real numbers,
          denoted by <m>{\mathbb R}^n</m>,
          form a vector space over <m>{\mathbb R}</m>.
          Given vectors <m>u = (u_1, \ldots,
          u_n)</m> and <m>v = (v_1, \ldots,
          v_n)</m> in <m>{\mathbb R}^n</m> and <m>\alpha</m> in <m>{\mathbb R}</m>,
          we can define vector addition by
          <me permid="KBW">
            u + v = (u_1, \ldots, u_n) + (v_1, \ldots, v_n) = (u_1 + v_1, \ldots, u_n + v_n)
          </me>
          and scalar multiplication by
          <me permid="qJf">
            \alpha u = \alpha(u_1, \ldots, u_n)= (\alpha u_1, \ldots, \alpha u_n)
          </me>.
        </p>
      </example>

      <example xml:id="example-vect-space-fx" permid="uXm">
        <p permid="Yrg">
          If <m>F</m> is a field, then <m>F[x]</m> is a vector space over <m>F</m>.
          The vectors in <m>F[x]</m> are simply polynomials,
          and vector addition is just polynomial addition.
          If <m>\alpha \in F</m> and <m>p(x) \in F[x]</m>,
          then scalar multiplication is defined by <m>\alpha p(x)</m>.
        </p>
      </example>

      <example xml:id="example-vect-space-cont-func" permid="bev">
        <p permid="Eyp">
          The set of all continuous real-valued functions on a closed interval <m>[a,b]</m> is a vector space over <m>{\mathbb R}</m>.
          If <m>f(x)</m> and <m>g(x)</m> are continuous on <m>[a, b]</m>,
          then <m>(f+g)(x)</m> is defined to be <m>f(x) + g(x)</m>.
          Scalar multiplication is defined by
          <m>(\alpha f)(x) = \alpha f(x)</m> for <m>\alpha \in {\mathbb R}</m>.
          For example, if <m>f(x) = \sin x</m> and <m>g(x)= x^2</m>,
          then <m>(2f + 5g)(x) =2 \sin x + 5 x^2</m>.
        </p>
      </example>

      <example xml:id="example-vect-space-sqrt2" permid="HlE">
        <p permid="kFy">
          Let <m>V = {\mathbb Q}(\sqrt{2}\, ) = \{ a + b \sqrt{2} : a, b \in {\mathbb Q } \}</m>.
          Then <m>V</m> is a vector space over <m>{\mathbb Q}</m>.
          If <m>u = a + b \sqrt{2}</m> and <m>v = c + d \sqrt{2}</m>,
          then <m>u + v = (a + c) + (b + d ) \sqrt{2}</m> is again in <m>V</m>.
          Also, for <m>\alpha \in {\mathbb Q}</m>,
          <m>\alpha v</m> is in <m>V</m>.
          We will leave it as an exercise to verify that all of the vector space axioms hold for <m>V</m>.
        </p>
      </example>

      <p>
        Just as we were able to prove some basic, fundamental facts about groups from the axioms, we can easily verify some facts for vector spaces.
      </p>

      <proposition permid="dZb">
        <statement>
          <p permid="HsV">
            Let <m>V</m> be a vector space over <m>F</m>.
            Then each of the following statements is true.

            <ol permid="pRU">
              <li permid="LzY">
                <p permid="orc">
                  <m>0v ={\mathbf 0}</m> for all <m>v \in V</m>.
                </p>
              </li>

              <li permid="rHh">
                <p permid="Uyl">
                  <m>\alpha {\mathbf 0} = {\mathbf 0}</m> for all <m>\alpha \in F</m>.
                </p>
              </li>

              <li permid="XOq">
                <p permid="AFu">
                  If <m>\alpha v = {\mathbf 0}</m>,
                  then either <m>\alpha = 0</m> or <m>v = {\mathbf 0}</m>.
                </p>
              </li>

              <li permid="DVz">
                <p permid="gMD">
                  <m>(-1) v = -v</m> for all <m>v \in V</m>.
                </p>
              </li>

              <li permid="kcI">
                <p permid="MTM">
                  <m>-(\alpha v) = (-\alpha)v = \alpha(-v)</m> for all
                  <m>\alpha \in F</m> and all <m>v \in V</m>.
                </p>
              </li>
            </ol>
          </p>
        </statement>

        <proof permid="YjP">
          <p permid="egs">
            To prove (1), observe that
            <me permid="WQo">
              0 v = (0 + 0)v = 0v + 0v;
            </me>
            consequently, <m>{\mathbf 0} + 0 v = 0v + 0v</m>.
            Since <m>V</m> is an abelian group, <m>{\mathbf 0} = 0v</m>.
          </p>

          <p permid="KnB">
            The proof of (2) is almost identical to the proof of (1).
            For (3), we are done if <m>\alpha = 0</m>.
            Suppose that <m>\alpha \neq 0</m>.
            Multiplying both sides of <m>\alpha v = {\mathbf 0}</m> by <m>1/ \alpha</m>,
            we have <m>v = {\mathbf 0}</m>.
          </p>

          <p permid="quK">
            To show (4), observe that
            <me permid="CXx">
              v + (-1)v = 1v + (-1)v = (1-1)v = 0v = {\mathbf 0}
            </me>,
            and so <m>-v = (-1)v</m>.
            We will leave the proof of (5) as an exercise.
          </p>
        </proof>
      </proposition>
    </subsubsection>

    <subsubsection xml:id="section-subspaces" permid="uns">
      <title>Subspaces</title>
      <p permid="NwC">
        Just as groups have subgroups and rings have subrings,
        vector spaces also have substructures.
        Let <m>V</m> be a vector space over a field <m>F</m>,
        and <m>W</m> a subset of <m>V</m>.
        Then <m>W</m> is a <term>subspace</term><idx><h>Vector space</h><h>subspace of</h></idx> of <m>V</m> if it is closed under vector addition and scalar multiplication;
        that is, if <m>u, v \in W</m> and <m>\alpha \in F</m>,
        it will always be the case that <m>u + v</m> and
        <m>\alpha v</m> are also in <m>W</m>.
      </p>

      <example xml:id="example-vect-subspace-w" permid="nsN">
        <p permid="QMH">
          Let <m>W</m> be the subspace of
          <m>{\mathbb R}^3</m> defined by <m>W = \{ (x_1, 2 x_1 + x_2, x_1 - x_2) : x_1, x_2 \in {\mathbb R} \}</m>.
          We claim that <m>W</m> is a  subspace of <m>{\mathbb R}^3</m>.
          Since
          <md permid="PlP">
            <mrow>\alpha (x_1, 2 x_1 + x_2, x_1 - x_2) &amp; =  (\alpha x_1, \alpha(2 x_1 + x_2), \alpha( x_1 - x_2))</mrow>
            <mrow>&amp; =  (\alpha x_1, 2(\alpha x_1) + \alpha x_2, \alpha x_1 -\alpha x_2)</mrow>
          </md>,
          <m>W</m> is closed under scalar multiplication.
          To show that <m>W</m> is closed under vector addition,
          let <m>u = (x_1, 2 x_1 + x_2, x_1 - x_2)</m> and
          <m>v = (y_1, 2 y_1 + y_2, y_1 - y_2)</m> be vectors in <m>W</m>.
          Then
          <me permid="jeG">
            u + v = (x_1 + y_1, 2( x_1 + y_1) +( x_2 + y_2), (x_1 + y_1) - (x_2+ y_2))
          </me>.
        </p>
      </example>

      <example xml:id="example-vect-subspace-poly" permid="TzW">
        <p permid="wTQ">
          Let <m>W</m> be the subset of polynomials of <m>F[x]</m> with no odd-power terms.
          If <m>p(x)</m> and <m>q(x)</m> have no odd-power terms,
          then neither will <m>p(x) + q(x)</m>.
          Also, <m>\alpha p(x) \in W</m> for
          <m>\alpha \in F</m> and <m>p(x) \in W</m>.
        </p>
      </example>
          <!--  2010/05/18 R Beezer, "vector field" to "vector space" -->
          
      <p>
        For groups and rings, a natural way to get a subgroup or subring was to look at the set of all elements <em>generated</em> by one or more elements.  For vector spaces, the set of elements (vectors) that are generated by a few particular elements is called a <term>spanning set</term>.
      </p>    
          
      <p permid="tDL">
        More in general, let <m>V</m> be any vector space over a field <m>F</m> and suppose that
        <m>v_1, v_2, \ldots,
        v_n</m> are vectors in <m>V</m> and
        <m>\alpha_1, \alpha_2, \ldots, \alpha_n</m> are scalars in <m>F</m>.
        Any vector <m>w</m> in <m>V</m> of the form
        <me permid="vsY">
          w = \sum_{i=1}^n \alpha_i v_i = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n
        </me>
        is called a <term>linear combination</term>
            <idx><h>Linear combination</h></idx>
        of the vectors <m>v_1, v_2, \ldots, v_n</m>.
        The <term>spanning set</term>
            <idx><h>Spanning set</h></idx>
        of vectors <m>v_1, v_2, \ldots,
        v_n</m> is the set of vectors obtained from all possible linear combinations of <m>v_1, v_2, \ldots, v_n</m>.
        If <m>W</m> is the spanning set of <m>v_1, v_2, \ldots, v_n</m>,
        then we say that <m>W</m> is <term>spanned</term>
        by <m>v_1, v_2, \ldots, v_n</m>.
      </p>

      <proposition permid="Kgk">
        <statement>
          <p permid="nAe">
            Let <m>S= \{v_1, v_2, \ldots,
            v_n \}</m> be vectors in a vector space <m>V</m>.
            Then the span of <m>S</m> is a subspace of <m>V</m>.
          </p>
        </statement>

        <proof permid="EqY">
          <p permid="WBT">
            Let <m>u</m> and <m>v</m> be in <m>S</m>.
            We can write both of these vectors as  linear combinations of the <m>v_i</m>'s:
            <md permid="nOz">
              <mrow>u &amp; =  \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n</mrow>
              <mrow>v &amp; =  \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n</mrow>
            </md>.
            Then
            <me permid="bAh">
              u + v =( \alpha_1 + \beta_1) v_1 + (\alpha_2+ \beta_2) v_2 + \cdots + (\alpha_n + \beta_n) v_n
            </me>
            is a linear combination of the <m>v_i</m>'s.
            For <m>\alpha \in F</m>,
            <me permid="HHq">
              \alpha u = (\alpha \alpha_1) v_1 + ( \alpha \alpha_2) v_2 + \cdots + (\alpha \alpha_n ) v_n
            </me>
            is in the span of <m>S</m>.
          </p>
        </proof>
      </proposition>
    </subsubsection>

    <subsubsection xml:id="section-linear-independence" permid="auB">
      <title>Linear Independence</title>
      <p permid="ZKU">
        Let <m>S = \{v_1, v_2, \ldots,
        v_n\}</m> be a set of vectors in a vector space <m>V</m>.
        If there exist scalars <m>\alpha_1, \alpha_2 \ldots \alpha_n \in F</m> such that not all of the <m>\alpha_i</m>'s are zero and
        <me permid="TVI">
          \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\mathbf 0 }
        </me>,
        then <m>S</m> is said to be <term>linearly dependent</term>.
            <idx><h>Linear dependence</h></idx>
        If the set <m>S</m> is not linearly dependent,
        then it is said to be <term>linearly independent</term>.
            <idx><h>Linear independence</h></idx>
        More specifically, <m>S</m> is a linearly independent set if
        <me permid="AcR">
          \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\mathbf 0 }
        </me>
        implies that
        <me permid="gka">
          \alpha_1 = \alpha_2 = \cdots = \alpha_n = 0
        </me>
        for any set of scalars <m>\{ \alpha_1, \alpha_2 \ldots \alpha_n \}</m>.
      </p>

      <proposition permid="qnt">
        <statement>
          <p permid="THn">
            Let <m>\{ v_1, v_2, \ldots,
            v_n \}</m> be a set of linearly independent vectors in a vector space.
            Suppose that
            <me permid="Mrj">
              v = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n
            </me>.
            Then <m>\alpha_1 = \beta_1, \alpha_2 = \beta_2, \ldots, \alpha_n = \beta_n</m>.
          </p>
        </statement>

        <proof permid="kyh">
          <p permid="CJc">
            If
            <me permid="sys">
              v = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n
            </me>,
            then
            <me permid="YFB">
              (\alpha_1 - \beta_1) v_1 + (\alpha_2 - \beta_2) v_2 + \cdots + (\alpha_n - \beta_n) v_n = {\mathbf 0}
            </me>.
            Since <m>v_1, \ldots, v_n</m> are linearly independent,
            <m>\alpha_i - \beta_i = 0</m> for <m>i = 1, \ldots, n</m>.
          </p>
        </proof>
      </proposition>

      <p permid="FSd">
        The definition of linear dependence makes more sense if we consider the following proposition.
      </p>

      <proposition permid="WuC">
        <statement>
          <p permid="zOw">
            A set <m>\{ v_1, v_2, \dots,
            v_n \}</m> of vectors in a vector space <m>V</m> is linearly dependent if and only if one of the <m>v_i</m>'s is a linear combination of the rest.
          </p>
        </statement>

        <proof permid="QFq">
          <p permid="iQl">
            Suppose that <m>\{ v_1, v_2, \dots,
            v_n \}</m> is a set of linearly dependent vectors.
            Then there exist scalars <m>\alpha_1, \ldots, \alpha_n</m> such that
            <me permid="EMK">
              \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\mathbf 0 }
            </me>,
            with at least one of the <m>\alpha_i</m>'s not equal to zero.
            Suppose that <m>\alpha_k \neq 0</m>.
            Then
            <me permid="kTT">
              v_k = - \frac{\alpha_1}{\alpha_k} v_1 - \cdots - \frac{\alpha_{k - 1}}{\alpha_k} v_{k-1} - \frac{\alpha_{k + 1}}{\alpha_k} v_{k + 1} - \cdots - \frac{\alpha_n}{\alpha_k} v_n
            </me>.
          </p>

          <p permid="OXu">
            Conversely, suppose that
            <me permid="Rbc">
              v_k = \beta_1 v_1 + \cdots + \beta_{k - 1} v_{k - 1} + \beta_{k + 1} v_{k + 1} + \cdots + \beta_n v_n
            </me>.
            Then
            <me permid="xil">
              \beta_1 v_1 + \cdots + \beta_{k - 1} v_{k - 1} - v_k + \beta_{k + 1} v_{k + 1} + \cdots + \beta_n v_n = {\mathbf 0}
            </me>.
          </p>
        </proof>
      </proposition>

      <p permid="lZm">
        The following proposition is a consequence of the fact that any system of homogeneous linear equations with more unknowns than equations will have a nontrivial solution.
        We leave the details of the proof for the end-of-chapter exercises.
      </p>

      <proposition xml:id="proposition-linearly-independent" permid="CBL">
        <statement>
          <p permid="fVF">
            Suppose that a vector space <m>V</m> is spanned by <m>n</m> vectors.
            If <m>m \gt n</m>,
            then any set of <m>m</m> vectors in <m>V</m> must be linearly dependent.
          </p>
        </statement>
      </proposition>

      <p permid="Sgv">
        A set <m>\{ e_1, e_2, \ldots,
        e_n \}</m> of vectors in a vector space <m>V</m> is called a
        <term>basis</term><idx><h>Vector space</h><h>basis of</h></idx> for <m>V</m> if
        <m>\{ e_1, e_2, \ldots,
        e_n \}</m> is a linearly independent set that spans <m>V</m>.
      </p>

      <example xml:id="example-vect-basis-r3" permid="zHf">
        <p permid="daZ">
          The vectors <m>e_1 = (1, 0, 0)</m>, <m>e_2 = (0, 1, 0)</m>,
          and <m>e_3 =(0, 0, 1)</m> form a basis for <m>{\mathbb R}^3</m>.
          The set certainly spans <m>{\mathbb R}^3</m>,
          since any arbitrary vector <m>(x_1, x_2, x_3)</m> in
          <m>{\mathbb R}^3</m> can be written as <m>x_1 e_1 + x_2 e_2 + x_3 e_3</m>.
          Also, none of the vectors <m>e_1, e_2, e_3</m> can be written as a linear combination of the other two;
          hence, they are linearly independent.
          The vectors <m>e_1, e_2, e_3</m> are not the only basis of <m>{\mathbb R}^3</m>:
          the set <m>\{ (3, 2, 1), (3, 2, 0), (1, 1, 1) \}</m> is also a basis for <m>{\mathbb R}^3</m>.
        </p>
      </example>

      <example xml:id="example-vect-basis-sqrt2" permid="fOo">
        <p permid="Jii">
          Let <m>{\mathbb Q}( \sqrt{2}\, ) = \{ a + b \sqrt{2} : a, b \in {\mathbb Q} \}</m>.
          The sets <m>\{1, \sqrt{2}\, \}</m> and
          <m>\{1 + \sqrt{2}, 1 - \sqrt{2}\, \}</m> are both bases of <m>{\mathbb Q}( \sqrt{2}\, )</m>.
        </p>
      </example>

      <p permid="ynE">
        From the last two examples it should be clear that a given vector space has several bases.
        In fact, there are an infinite number of bases for both of these examples. <em>In general,
        there is no unique basis for a vector space.</em> However,
        every basis of <m>{\mathbb R}^3</m> consists of exactly three vectors,
        and every basis of <m>{\mathbb Q}(\sqrt{2}\, )</m> consists of exactly two vectors.
        This is a consequence of the next proposition.
      </p>

      <proposition permid="iIU">
        <statement>
          <p permid="McO">
            Let <m>\{ e_1, e_2, \ldots, e_m \}</m> and
            <m>\{ f_1, f_2, \ldots,
            f_n \}</m> be two bases for a vector space <m>V</m>.
            Then <m>m = n</m>.
          </p>
        </statement>

        <proof permid="wMz">
          <p permid="veD">
            Since <m>\{ e_1, e_2, \ldots,
            e_m \}</m> is a basis, it is a linearly independent set.
            By <xref ref="proposition-linearly-independent"/>, <m>n \leq m</m>.
            Similarly, <m>\{ f_1, f_2, \ldots,
            f_n \}</m> is a linearly independent set,
            and the last proposition implies that <m>m \leq n</m>.
            Consequently, <m>m = n</m>.
          </p>
        </proof>
      </proposition>
          <!-- Label repaired.  Suggested by R. Beezer. -->
          <!-- TWJ - 12/19/2011 -->
      <p permid="euN">
        If <m>\{ e_1, e_2, \ldots, e_n \}</m> is a basis for a vector space <m>V</m>,
        then we say that the <term>dimension</term><idx><h>Vector space</h><h>dimension of</h></idx> of <m>V</m> is <m>n</m> and we write <m>\dim V =n</m>.

        <notation>
          <usage>\dim V</usage>
          <description>dimension of a vector space <m>V</m></description>
        </notation>

        We will leave the proof of the following theorem as an exercise.
      </p>

      <theorem permid="xRS">
        <statement>
          <p permid="blM">
            Let <m>V</m> be a vector space of dimension <m>n</m>.

            <ol permid="JKL">
              <li permid="QjR">
                <p permid="taV">
                  If <m>S = \{v_1, \ldots,
                  v_n \}</m> is a set of linearly independent vectors for <m>V</m>,
                  then <m>S</m> is a basis for <m>V</m>.
                </p>
              </li>

              <li permid="wra">
                <p permid="Zie">
                  If <m>S = \{v_1, \ldots,
                  v_n \}</m> spans <m>V</m>, then <m>S</m> is a basis for <m>V</m>.
                </p>
              </li>

              <li permid="cyj">
                <p permid="Fpn">
                  If <m>S = \{v_1, \ldots,
                  v_k \}</m> is a set of linearly independent vectors for <m>V</m> with <m>k \lt n</m>,
                  then there exist vectors <m>v_{k + 1}, \ldots, v_n</m> such that
                  <me permid="dpu">
                    \{v_1, \ldots, v_k, v_{k + 1}, \ldots, v_n \}
                  </me>
                  is a basis for <m>V</m>.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </theorem>

    </subsubsection>
    
    
    
  </subsection>
  
  <xi:include href="worksheets/fields-extension-degree.ptx" />
  <xi:include href="worksheets/fields-extension-iterated.ptx" />
  
  <subsection xml:id="subsec-fields-vectorspaces-degree">
    <title>Back to Extension Fields</title>
    
    <p permid="qbP">
      Let <m>E</m> be a field extension of a field <m>F</m>.
      If we regard <m>E</m> as a vector space over <m>F</m>,
      then we can bring the machinery of linear algebra to bear on the problems that we will encounter in our study of fields.
      The elements in the field <m>E</m> are vectors;
      the elements in the field <m>F</m> are scalars.
      We can think of addition in <m>E</m> as adding vectors.
      When we multiply an element in <m>E</m> by an element of <m>F</m>,
      we are multiplying a vector by a scalar.
      This view of field extensions is especially fruitful if a field extension <m>E</m> of <m>F</m> is a finite dimensional vector space over <m>F</m>,
      and <xref ref="theorem-simple_extension"/> states that
      <m>E = F(\alpha )</m> is finite dimensional vector space over <m>F</m> with basis <m>\{ 1, \alpha, {\alpha}^2, \ldots, {\alpha}^{n - 1} \}</m>.
    </p>

    <p permid="WiY">
      If an extension field <m>E</m> of a field <m>F</m> is a finite dimensional vector space over <m>F</m> of dimension <m>n</m>,
      then we say that <m>E</m> is a 
      <term>finite extension of degree <m>n</m> over <m>F</m></term>.
          <idx><h>Extension</h><h>finite</h></idx>
      We write
      <me permid="APp">
        [E:F]= n
      </me>.
      to indicate the dimension of <m>E</m> over <m>F</m>.

      <notation>
        <usage>[E:F]</usage>
        <description>dimension of a field extension of <m>E</m> over <m>F</m></description>
      </notation>

    </p>

    <theorem xml:id="theorem-finite-extension" permid="hdp">
      <statement>
        <p permid="JGl">
          Every finite extension field <m>E</m> of a field <m>F</m> is an algebraic extension.
        </p>
      </statement>

      <proof permid="Efh">
        <p permid="ZUC">
          Let <m>\alpha \in E</m>.
          Since <m>[E:F] = n</m>, the elements
          <me permid="gWy">
            1, \alpha, \ldots, {\alpha}^n
          </me>
          cannot be linearly independent.
          Hence, there exist <m>a_i \in F</m>, not all zero, such that
          <me permid="NdH">
            a_n {\alpha}^n + a_{n - 1} {\alpha}^{n - 1} + \cdots + a_1 \alpha + a_0 = 0
          </me>.
          Therefore,
          <me permid="tkQ">
            p(x) = a_n x^n + \cdots + a_0 \in F[x]
          </me>
          is a nonzero polynomial with <m>p( \alpha ) = 0</m>.
        </p>
      </proof>
    </theorem>

    <remark permid="CLt">
      <p permid="eiW">
        <xref ref="theorem-finite-extension"/> says that every finite extension of a field <m>F</m> is an algebraic extension.
        The converse is false, however.
        We will leave it as an exercise to show that the set of all elements in
        <m>{\mathbb R}</m> that are algebraic over
        <m>{\mathbb Q}</m> forms an infinite field extension of <m>{\mathbb Q}</m>.
      </p>
    </remark>

    <p permid="Cqh">
      The next theorem is a counting theorem,
      similar to Lagrange's Theorem in group theory.
      <xref ref="theorem-tower-indices"/> will prove to be an extremely useful tool in our investigation of finite field extensions.
    </p>
          <!--Changed the refrerence in the preceding paragraph to the next theorem instead of the previous theorem.  TWJ 13/6/2014-->
    <theorem xml:id="theorem-tower-indices" permid="Nky">
      <statement>
        <p permid="pNu">
          If <m>E</m> is a finite extension of <m>F</m> and <m>K</m> is a finite extension of <m>E</m>,
          then <m>K</m> is a finite extension of <m>F</m> and
          <me permid="ZrZ">
            [K:F]= [K:E] [E:F]
          </me>.
        </p>
      </statement>

      <proof permid="kmq">
        <p permid="GbL">
          Let <m>\{ \alpha_1, \ldots, \alpha_n \}</m> be a basis for <m>E</m> as a vector space over <m>F</m> and
          <m>\{ \beta_1, \ldots, \beta_m \}</m> be a basis for <m>K</m> as a vector space over <m>E</m>.
          We claim that <m>\{ \alpha_i \beta_j \}</m> is a basis for <m>K</m> over <m>F</m>.
          We will first show that these vectors span <m>K</m>.
          Let <m>u \in K</m>.
          Then <m>u = \sum_{j = 1}^{m} b_j \beta_j</m> and <m>b_j = \sum_{i = 1}^{n} a_{ij} \alpha_i</m>,
          where <m>b_j \in E</m> and <m>a_{ij} \in F</m>.
          Then
          <me permid="Fzi">
            u = \sum_{j = 1}^{m} \left( \sum_{i = 1}^{n} a_{ij} \alpha_i  \right) \beta_j = \sum_{i,j} a_{ij} ( \alpha_i \beta_j )
          </me>.
          So the <m>mn</m> vectors <m>\alpha_i \beta_j</m> must span <m>K</m> over <m>F</m>.
        </p>

        <p permid="miU">
          We must show that <m>\{ \alpha_i \beta_j \}</m> are linearly independent.
          Recall that a set of vectors
          <m>v_1, v_2, \ldots,
          v_n</m> in a vector space <m>V</m> are linearly independent if
          <me permid="lGr">
            c_1 v_1 + c_2 v_2 + \cdots + c_n v_n = 0
          </me>
          implies that
          <me permid="RNA">
            c_1 = c_2 = \cdots = c_n = 0
          </me>.
          Let
          <me permid="xUJ">
            u = \sum_{i,j} c_{ij} ( \alpha_i \beta_j ) = 0
          </me>
          for <m>c_{ij} \in F</m>.
          We need to prove that all of the <m>c_{ij}</m>'s are zero.
          We can rewrite <m>u</m> as
          <me permid="ebS">
            \sum_{j = 1}^{m} \left( \sum_{i = 1}^{n} c_{ij} \alpha_i \right) \beta_j = 0
          </me>,
          where <m>\sum_i c_{ij} \alpha_i \in E</m>.
          Since the <m>\beta_j</m>'s are linearly independent over <m>E</m>,
          it must be the case that
          <me permid="Kjb">
            \sum_{i = 1}^n c_{ij} \alpha_i = 0
          </me>
          for all <m>j</m>.
          However, the <m>\alpha_j</m> are also linearly independent  over <m>F</m>.
          Therefore, <m>c_{ij} = 0</m> for all <m>i</m> and <m>j</m>,
          which completes the proof.
        </p>
      </proof>
    </theorem>

    <p permid="ixq">
      The following corollary is easily proved using mathematical induction.
    </p>

    <corollary permid="FFZ">
      <statement>
        <p permid="DQZ">
          If <m>F_i</m> is a field for
          <m>i = 1, \dots,
          k</m> and <m>F_{i+1}</m> is a finite extension of <m>F_i</m>,
          then <m>F_k</m> is a finite extension of <m>F_1</m> and
          <me permid="qqk">
            [F_k : F_1] = [F_k : F_{k-1} ] \cdots [F_2 : F_1 ]
          </me>.
        </p>
      </statement>
    </corollary>

    <corollary permid="lNi">
      <statement>
        <p permid="jYi">
          Let <m>E</m> be an extension field of <m>F</m>.
          If <m>\alpha \in E</m> is algebraic over <m>F</m> with minimal polynomial <m>p(x)</m> and
          <m>\beta \in F( \alpha )</m> with minimal polynomial <m>q(x)</m>,
          then <m>\deg q(x)</m> divides <m>\deg p(x)</m>.
        </p>
      </statement>

      <proof permid="FUu">
        <p permid="Sqd">
          We know that <m>\deg p(x) = [F( \alpha ) : F ]</m> and <m>\deg q(x) = [F( \beta ) : F ]</m>.
          Since <m>F \subset F( \beta ) \subset F( \alpha )</m>,
          <me permid="Wxt">
            [F( \alpha ) : F ]= [ F( \alpha ) : F( \beta ) ] [ F( \beta ) : F ]
          </me>.
        </p>
      </proof>
    </corollary>

    <example xml:id="example-fields-sqrt3-sqrt5" permid="qxb">
      <p permid="FGm">
        Let us determine an extension field of
        <m>{\mathbb Q}</m> containing <m>\sqrt{3} + \sqrt{5}</m>.
        It is easy to determine that the minimal  polynomial of
        <m>\sqrt{3} + \sqrt{5}</m> is <m>x^4 - 16 x^2 + 4</m>.
        It follows that
        <me permid="CEC">
          [{\mathbb Q}( \sqrt{3} + \sqrt{5}\, ) : {\mathbb Q} ] = 4
        </me>.
        We know that <m>\{ 1, \sqrt{3}\, \}</m> is a basis for 
        <m>{\mathbb Q}( \sqrt{3}\, )</m> over <m>{\mathbb Q}</m>.
        Hence, <m>\sqrt{3} + \sqrt{5}</m> cannot be in <m>{\mathbb Q}( \sqrt{3}\, )</m>.
        It follows that <m>\sqrt{5}</m> cannot be in <m>{\mathbb Q}( \sqrt{3}\, )</m> either.
        Therefore, <m>\{ 1, \sqrt{5}\, \}</m> is a basis for <m>{\mathbb Q}( \sqrt{3}, \sqrt{5}\, ) = ( {\mathbb Q}(\sqrt{3}\, ))( \sqrt{5}\, )</m> over
        <m>{\mathbb Q}( \sqrt{3}\, )</m> and <m>\{ 1, \sqrt{3}, \sqrt{5}, \sqrt{3} \sqrt{5} = \sqrt{15}\, \}</m> is a basis for
        <m>{\mathbb Q}( \sqrt{3}, \sqrt{5}\, ) = {\mathbb Q}( \sqrt{3} + \sqrt{5}\, )</m> over <m>{\mathbb Q}</m>.
        This example shows that it is possible that some extension
        <m>F( \alpha_1, \ldots, \alpha_n )</m> is actually a simple extension of <m>F</m> even though <m>n \gt 1</m>.
      </p>
    </example>
          <!-- Changed minimal polynomial from  <m>x^4 - 16 x + 4</m> to <m>x^4 - 16 x^2 + 4</m>.  Discovered by Bradley Noyes - TWJ 3/25/2011 -->
    <example xml:id="example-fields-cubert5-sqrt5-i" permid="WEk">
      <p permid="lNv">
        Let us compute a basis for <m>{\mathbb Q}( \sqrt[3]{5}, \sqrt{5} \, i )</m>,
        where <m>\sqrt{5}</m> is the positive square root of <m>5</m> and
        <m>\sqrt[3]{5}</m> is the real cube root of <m>5</m>.
        We know that <m>\sqrt{5} \, i \notin {\mathbb Q}(\sqrt[3]{5}\, )</m>, so
        <me permid="iLL">
          [ {\mathbb Q}(\sqrt[3]{5}, \sqrt{5}\, i) : {\mathbb Q}(\sqrt[3]{5}\, )] = 2
        </me>.
        It is easy to determine that <m>\{ 1, \sqrt{5}i\, \}</m> is a basis for
        <m>{\mathbb Q}( \sqrt[3]{5}, \sqrt{5}\, i )</m> over <m>{\mathbb Q}( \sqrt[3]{5}\, )</m>.
        We also know that <m>\{ 1, \sqrt[3]{5}, (\sqrt[3]{5}\, )^2  \}</m> is a basis for
        <m>{\mathbb Q}(\sqrt[3]{5}\, )</m> over <m>{\mathbb Q}</m>.
        Hence, a basis for <m>{\mathbb Q}(\sqrt[3]{5}, \sqrt{5}\, i )</m> over <m>{\mathbb Q}</m> is
        <me permid="OSU">
          \{ 1, \sqrt{5}\, i, \sqrt[3]{5}, (\sqrt[3]{5}\, )^2, (\sqrt[6]{5}\, )^5 i, (\sqrt[6]{5}\, )^7 i = 5 \sqrt[6]{5}\, i \text{ or } \sqrt[6]{5}\, i \}
        </me>.
        Notice that <m>\sqrt[6]{5}\, i</m> is a zero of <m>x^6 + 5</m>.
        We can show that this polynomial is irreducible over
        <m>{\mathbb Q}</m> using Eisenstein's Criterion,
        where we let <m>p = 5</m>.
        Consequently,
        <me permid="vad">
          {\mathbb Q} \subset {\mathbb Q}( \sqrt[6]{5}\, i) \subset {\mathbb Q}( \sqrt[3]{5}, \sqrt{5}\, i )
        </me>.
        But it must be the case that <m>{\mathbb Q}( \sqrt[6]{5}\, i) = {\mathbb Q}( \sqrt[3]{5}, \sqrt{5}\, i )</m>,
        since the degree of both of these extensions is <m>6</m>.
      </p>
    </example>
          <!-- Corrected typo in the last set inclusion.  Suggest2/9/2013 -->
    <theorem permid="trH">
      <statement>
        <p permid="VUD">
          Let <m>E</m> be a field extension of <m>F</m>.
          Then  the following statements are equivalent.

          <ol permid="UHT">
            <li permid="KiO">
              <p permid="xyX">
                <m>E</m> is a finite extension of <m>F</m>.
              </p>
            </li>

            <li permid="qpX">
              <p permid="dGg">
                There exists a finite number of algebraic elements
                <m>\alpha_1, \ldots, \alpha_n \in E</m> such that <m>E = F(\alpha_1, \ldots, \alpha_n)</m>.
              </p>
            </li>

            <li permid="Wxg">
              <p permid="JNp">
                There exists a sequence of fields
                <me permid="bhm">
                  E = F(\alpha_1, \ldots, \alpha_n) \supset F(\alpha_1, \ldots, \alpha_{n-1} ) \supset \cdots \supset F( \alpha_1 ) \supset F
                </me>,
                where each field <m>F(\alpha_1, \ldots, \alpha_i)</m> is algebraic over <m>F(\alpha_1, \ldots, \alpha_{i-1})</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>

      <proof permid="Qtz">
        <p permid="yxm">
          (1) <m>\Rightarrow</m> (2).
          Let <m>E</m> be a finite algebraic extension of <m>F</m>.
          Then <m>E</m> is a finite dimensional vector space over <m>F</m> and there exists a basis consisting of elements
          <m>\alpha_1, \ldots, \alpha_n</m> in <m>E</m> such that <m>E = F(\alpha_1, \ldots, \alpha_n)</m>.
          Each <m>\alpha_i</m> is algebraic over <m>F</m> by <xref ref="theorem-finite-extension"/>.
        </p>

        <p permid="eEv">
          (2) <m>\Rightarrow</m> (3).
          Suppose that <m>E = F(\alpha_1, \ldots, \alpha_n)</m>,
          where every <m>\alpha_i</m> is algebraic over <m>F</m>.
          Then
          <me permid="Hov">
            E = F(\alpha_1, \ldots, \alpha_n) \supset F(\alpha_1, \ldots, \alpha_{n - 1} ) \supset \cdots \supset F( \alpha_1 ) \supset F
          </me>,
          where each field <m>F(\alpha_1, \ldots, \alpha_i)</m> is algebraic over <m>F(\alpha_1, \ldots, \alpha_{i - 1})</m>.
        </p>

        <p permid="KLE">
          (3) <m>\Rightarrow</m> (1).
          Let
          <me permid="nvE">
            E = F(\alpha_1, \ldots, \alpha_n) \supset F(\alpha_1, \ldots, \alpha_{n - 1} ) \supset \cdots \supset F( \alpha_1 ) \supset F
          </me>,
          where each field <m>F(\alpha_1, \ldots, \alpha_i)</m> is algebraic over <m>F(\alpha_1, \ldots, \alpha_{i - 1})</m>.
          Since
          <me permid="TCN">
            F(\alpha_1, \ldots, \alpha_i) =  F(\alpha_1, \ldots, \alpha_{i - 1} )(\alpha_i)
          </me>
          is simple extension and <m>\alpha_i</m> is algebraic over <m>F(\alpha_1, \ldots, \alpha_{i - 1})</m>,
          it follows that
          <me permid="zJW">
            [ F(\alpha_1, \ldots, \alpha_i) : F(\alpha_1, \ldots, \alpha_{i - 1} )]
          </me>
          is finite for each <m>i</m>.
          Therefore, <m>[E : F]</m> is finite.
        </p>
      </proof>
    </theorem>
  </subsection>

  <subsection xml:id="fields-subsection-algebraic-closure" permid="Cbz">
    <title>Algebraic Closure</title>
    <p permid="OEz">
      Given a field <m>F</m>,
      the question arises as to whether or not we can find a field <m>E</m> such that every polynomial <m>p(x)</m> has a root in <m>E</m>.
      This leads us to the following theorem.
    </p>

    <theorem permid="iSC">
      <statement>
        <p permid="CbM">
          Let <m>E</m> be an extension field of <m>F</m>.
          The set of elements in <m>E</m> that are algebraic over <m>F</m> form a field.
        </p>
      </statement>
          <!--  Changed <m>\alpha \pm \beta</m>, $\alpha / -->
          <!-- \beta<m>, and </m>\alpha / \beta<m> to </m>\alpha \pm \beta<m>, </m>\alpha  -->
          <!-- \beta<m>, and </m>\alpha / \beta$  Suggested by Aleks Vlasev. - TWJ 8/10/2011 -->
      <proof permid="wAI">
        <p permid="qSN">
          Let <m>\alpha, \beta \in E</m> be algebraic over <m>F</m>.
          Then <m>F( \alpha, \beta )</m> is a finite extension of <m>F</m>.
          Since every element of <m>F( \alpha, \beta )</m> is algebraic over <m>F</m>,
          <m>\alpha \pm \beta</m>, <m>\alpha \beta</m>,
          and <m>\alpha / \beta</m> (<m>\beta \neq 0</m>) are all algebraic over <m>F</m>.
          Consequently,
          the set of elements in <m>E</m> that are algebraic over <m>F</m> form a field.
        </p>
      </proof>
    </theorem>

    <corollary xml:id="corollary-algebraic-numbers-field" permid="Hvm">
      <statement>
        <p permid="Qfr">
          The set of all algebraic numbers forms a field;
          that is, the set of all complex numbers that are algebraic over <m>{\mathbb Q}</m> makes up a field.
        </p>
      </statement>
    </corollary>

    <p permid="uLI">
      Let <m>E</m> be a field extension of a field <m>F</m>.
      We define the <term>algebraic closure</term>
          <idx><h>Algebraic closure</h></idx>
      of a field <m>F</m> in <m>E</m> to be the field consisting of all elements in <m>E</m> that are algebraic over <m>F</m>.
      A field <m>F</m> is <term>algebraically closed</term><idx><h>Field</h><h>algebraically closed</h></idx> if every nonconstant polynomial in <m>F[x]</m> has a root in <m>F</m>.
    </p>

    <theorem permid="OZL">
      <statement>
        <p permid="iiV">
          A field <m>F</m> is algebraically closed if and only if every nonconstant polynomial in <m>F[x]</m> factors into linear factors over <m>F[x]</m>.
        </p>
      </statement>

      <proof permid="cHR">
        <p permid="WZW">
          Let <m>F</m> be an algebraically closed field.
          If <m>p(x) \in F[x]</m> is a nonconstant polynomial,
          then <m>p(x)</m> has a zero in <m>F</m>, say <m>\alpha</m>.
          Therefore, <m>x-\alpha</m> must be a factor of <m>p(x)</m> and so <m>p(x) = (x - \alpha) q_1(x)</m>,
          where <m>\deg q_1(x) = \deg p(x) - 1</m>.
          Continue this process with <m>q_1(x)</m> to find a factorization
          <me permid="fRf">
            p(x) = (x - \alpha)(x - \beta)q_2(x)
          </me>,
          where <m>\deg q_2(x) = \deg p(x) -2</m>.
          The process must eventually stop since the degree of <m>p(x)</m> is finite.
        </p>

        <p permid="Dhf">
          Conversely, suppose that every nonconstant polynomial <m>p(x)</m> in <m>F[x]</m> factors into linear factors.
          Let <m>ax - b</m> be such a factor.
          Then <m>p( b/a ) = 0</m>.
          Consequently, <m>F</m> is algebraically closed.
        </p>
      </proof>
    </theorem>

    <corollary permid="nCv">
      <statement>
        <p permid="wmA">
          An algebraically closed field <m>F</m> has no proper algebraic extension <m>E</m>.
        </p>
      </statement>

      <proof permid="mbD">
        <p permid="joo">
          Let <m>E</m> be an algebraic extension of <m>F</m>;
          then <m>F \subset E</m>.
          For <m>\alpha \in E</m>,
          the minimal polynomial of <m>\alpha</m> is <m>x - \alpha</m>.
          Therefore, <m>\alpha \in F</m> and <m>F = E</m>.
        </p>
      </proof>
    </corollary>

    <theorem permid="vgU">
      <statement>
        <p permid="Oqe">
          Every field <m>F</m> has a unique algebraic closure.
        </p>
      </statement>
    </theorem>

    <p permid="aSR">
      It is a nontrivial fact that every field has a unique algebraic closure.
      The proof is not extremely difficult,
      but requires some rather sophisticated set theory.
      We refer the reader to [3], [4], or [8] for a proof of this result.
    </p>
          <!-- not sure how to number these references within the chapter - 19 August 2010 - TWJ -->
    <p permid="Haa">
      We now state the Fundamental Theorem of Algebra,
      first proven by Gauss at the age of 22 in his doctoral thesis.
      This theorem states that every polynomial with coefficients in the complex numbers has a root in the complex numbers.
      The proof of this theorem will be given in <xref ref="galois"/>.
    </p>

    <theorem permid="bod">
      <title>Fundamental Theorem of Algebra</title>
      <idx>
      <h>Fundamental Theorem</h>
      <h>of Algebra</h>
      </idx>
      <statement>
        <p permid="uxn">
          The field of complex numbers is algebraically closed.
        </p>
      </statement>
    </theorem>
  </subsection>
  
  
</section>